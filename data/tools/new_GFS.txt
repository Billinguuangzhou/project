We have designed and implemented the Google File System, a scalable distributed ﬁle system for large distributed data-intensive applications. It provides fault tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients.
While sharing many of the same goals as previous distributed ﬁle systems, our design has been driven by observations of our application workloads and technological environment, both current and anticipated, that reﬂect a marked departure from some earlier ﬁle system assumptions. This has led us to reexamine traditional choices and explore radically diﬀerent design points.
The ﬁle system has successfully met our storage needs. by hundreds of clients.
In this paper, we present ﬁle system interface extensions designed to support distributed applications, discuss many aspects of our design, and report measurements from both micro-benchmarks and real world use.
Fault tolerance, scalability, data storage, clustered storage ∗ The authors can be reached at the following addresses: {sanjay,hgobioﬀ,shuntak}@google.com.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. SOSP’03, October 19–22, 2003, Bolton Landing, New York, USA. Copyright 2003 ACM 1-58113-757-5/03/0010 ...$5.00.
We have designed and implemented the Google File System (GFS) to meet the rapidly growing demands of Google’s data processing needs. GFS shares many of the same goals as previous distributed ﬁle systems such as performance, scalability, reliability, and availability. However, its design has been driven by key observations of our application workloads and technological environment, both current and anticipated, that reﬂect a marked departure from some earlier ﬁle system design assumptions. We have reexamined traditional choices and explored radically diﬀerent points in the design space.
First, component failures are the norm rather than the exception. The ﬁle system consists of hundreds or even thousands of storage machines built from inexpensive commodity parts and is accessed by a comparable number of client machines. The quantity and quality of the components virtually guarantee that some are not functional at any given time and some will not recover from their current failures. We have seen problems caused by application bugs, operating system bugs, human errors, and the failures of disks, memory, connectors, networking, and power supplies. Therefore, constant monitoring, error detection, fault tolerance, and automatic recovery must be integral to the system.
Second, ﬁles are huge by traditional standards. Multi-GB ﬁles are common. Each ﬁle typically contains many application objects such as web documents. When we are regularly working with fast growing data sets of many TBs comprising billions of objects, it is unwieldy to manage billions of approximately KB-sized ﬁles even when the ﬁle system could support it. As a result, design assumptions and parameters such as I/O operation and block sizes have to be revisited. while caching data blocks in the client loses its appeal.
Fourth, co-designing the applications and the ﬁle system API beneﬁts the overall system by increasing our ﬂexibility.
For example, we have relaxed GFS’s consistency model to vastly simplify the ﬁle system without imposing an onerous burden on the applications. We have also introduced an atomic append operation so that multiple clients can append concurrently to a ﬁle without extra synchronization between them. These will be discussed in more details later in the paper.
Multiple GFS clusters are currently deployed for diﬀerent purposes. The largest ones have over 1000 storage nodes, over 300 TB of disk storage, and are heavily accessed by hundreds of clients on distinct machines on a continuous basis.
In designing a ﬁle system for our needs, we have been guided by assumptions that oﬀer both challenges and opportunities. We alluded to some key observations earlier and now lay out our assumptions in more details.
(cid:127) The system is built from many inexpensive commodity components that often fail. It must constantly monitor itself and detect, tolerate, and recover promptly from component failures on a routine basis.
(cid:127) The system stores a modest number of large ﬁles. We expect a few million ﬁles, each typically 100 MB or larger in size. Multi-GB ﬁles are the common case and should be managed eﬃciently. Small ﬁles must be supported, but we need not optimize for them.
(cid:127) The workloads primarily consist of two kinds of reads: large streaming reads and small random reads. read hundreds of KBs, more commonly 1 MB or more. the ﬁle rather than go back and forth.
(cid:127) The workloads also have many large, sequential writes that append data to ﬁles. Typical operation sizes are similar to those for reads. Once written, ﬁles are seldom modiﬁed again. Small writes at arbitrary positions in a ﬁle are supported but do not have to be eﬃcient.
(cid:127) The system must eﬃciently implement well-deﬁned semantics for multiple clients that concurrently append to the same ﬁle. Our ﬁles are often used as producerconsumer queues or for many-way merging. Hundreds of producers, running one per machine, will concurrently append to a ﬁle. Atomicity with minimal synchronization overhead is essential. The ﬁle may be read later, or a consumer may be reading through the ﬁle simultaneously.
(cid:127) High sustained bandwidth is more important than low latency. Most of our target applications place a premium on processing data in bulk at a high rate, while few have stringent response time requirements for an individual read or write.
GFS provides a familiar ﬁle system interface, though it does not implement a standard API such as POSIX. Files are organized hierarchically in directories and identiﬁed by pathnames. We support the usual operations to create, delete, open, close, read, and write ﬁles.
Moreover, GFS has snapshot and record append operations. Snapshot creates a copy of a ﬁle or a directory tree at low cost. Record append allows multiple clients to append data to the same ﬁle concurrently while guaranteeing the atomicity of each individual client’s append. It is useful for implementing multi-way merge results and producerconsumer queues that many clients can simultaneously append to without additional locking. We have found these types of ﬁles to be invaluable in building large distributed applications. Snapshot and record append are discussed further in Sections 3.4 and 3.3 respectively.
A GFS cluster consists of a single master and multiple chunkservers and is accessed by multiple clients, as shown in Figure 1. Each of these is typically a commodity Linux machine running a user-level server process. It is easy to run both a chunkserver and a client on the same machine, as long as machine resources permit and the lower reliability caused by running possibly ﬂaky application code is acceptable.
Files are divided into ﬁxed-size chunks. Each chunk is identiﬁed by an immutable and globally unique 64 bit chunk handle assigned by the master at the time of chunk creation. regions of the ﬁle namespace.
The master maintains all ﬁle system metadata. This includes the namespace, access control information, the mapping from ﬁles to chunks, and the current locations of chunks. messages to give it instructions and collect its state.
GFS client code linked into each application implements the ﬁle system API and communicates with the master and chunkservers to read or write data on behalf of the application. Clients interact with the master for metadata operations, but all data-bearing communication goes directly to the chunkservers. We do not provide the POSIX API and therefore need not hook into the Linux vnode layer.
Neither the client nor the chunkserver caches ﬁle data. the overall system by eliminating cache coherence issues. data in memory.
Having a single master vastly simpliﬁes our design and enables the master to make sophisticated chunk placement
and replication decisions using global knowledge. However, we must minimize its involvement in reads and writes so that it does not become a bottleneck. Clients never read and write ﬁle data through the master. Instead, a client asks the master which chunkservers it should contact. It caches this information for a limited time and interacts with the chunkservers directly for many subsequent operations.
Let us explain the interactions for a simple read with reference to Figure 1. First, using the ﬁxed chunk size, the client translates the ﬁle name and byte oﬀset speciﬁed by the application into a chunk index within the ﬁle. Then, it sends the master a request containing the ﬁle name and chunk index. The master replies with the corresponding chunk handle and locations of the replicas. The client caches this information using the ﬁle name and chunk index as the key. until the cached information expires or the ﬁle is reopened. teractions at practically no extra cost.
Chunk size is one of the key design parameters. We have chosen 64 MB, which is much larger than typical ﬁle system block sizes. Each chunk replica is stored as a plain Linux ﬁle on a chunkserver and is extended only as needed. a large chunk size.
tent TCP connection to the chunkserver over an extended period of time. Third, it reduces the size of the metadata stored on the master. This allows us to keep the metadata in memory, which in turn brings other advantages that we will discuss in Section 2.6.1.
On the other hand, a large chunk size, even with lazy space allocation, has its disadvantages. A small ﬁle consists of a small number of chunks, perhaps just one. The chunkservers storing those chunks may become hot spots if many clients are accessing the same ﬁle. In practice, hot spots have not been a major issue because our applications mostly read large multi-chunk ﬁles sequentially.
However, hot spots did develop when GFS was ﬁrst used by a batch-queue system: an executable was written to GFS as a single-chunk ﬁle and then started on hundreds of machines at the same time. The few chunkservers storing this executable were overloaded by hundreds of simultaneous requests. We ﬁxed this problem by storing such executables with a higher replication factor and by making the batchqueue system stagger application start times. A potential long-term solution is to allow clients to read data from other clients in such situations.
The master stores three major types of metadata: the ﬁle and chunk namespaces, the mapping from ﬁles to chunks, and the locations of each chunk’s replicas. All metadata is kept in the master’s memory. The ﬁrst two types (namespaces and ﬁle-to-chunk mapping) are also kept persistent by logging mutations to an operation log stored on the master’s local disk and replicated on remote machines. Using a log allows us to update the master state simply, reliably, and without risking inconsistencies in the event of a master crash. The master does not store chunk location information persistently. Instead, it asks each chunkserver about its chunks at master startup and whenever a chunkserver joins the cluster.
Since metadata is stored in memory, master operations are fast. Furthermore, it is easy and eﬃcient for the master to periodically scan through its entire state in the background.
usage across chunkservers. Sections 4.3 and 4.4 will discuss these activities further.
One potential concern for this memory-only approach is that the number of chunks and hence the capacity of the whole system is limited by how much memory the master has. This is not a serious limitation in practice. The master maintains less than 64 bytes of metadata for each 64 MB chunk. Most chunks are full because most ﬁles contain many chunks, only the last of which may be partially ﬁlled. Similarly, the ﬁle namespace data typically requires less then 64 bytes per ﬁle because it stores ﬁle names compactly using preﬁx compression.
If necessary to support even larger ﬁle systems, the cost of adding extra memory to the master is a small price to pay for the simplicity, reliability, performance, and ﬂexibility we gain by storing the metadata in memory.
The master does not keep a persistent record of which chunkservers have a replica of a given chunk. It simply polls chunkservers for that information at startup. The master can keep itself up-to-date thereafter because it controls all chunk placement and monitors chunkserver status with regular HeartBeat messages.
We initially attempted to keep chunk location information persistently at the master, but we decided that it was much simpler to request the data from chunkservers at startup, and periodically thereafter. This eliminated the problem of keeping the master and chunkservers in sync as chunkservers join and leave the cluster, change names, fail, restart, and so on. In a cluster with hundreds of servers, these events happen all too often.
Another way to understand this design decision is to realize that a chunkserver has the ﬁnal word over what chunks it does or does not have on its own disks. There is no point in trying to maintain a consistent view of this information on the master because errors on a chunkserver may cause chunks to vanish spontaneously (e.g., a disk may go bad and be disabled) or an operator may rename a chunkserver.
The operation log contains a historical record of critical metadata changes. It is central to GFS. Not only is it the only persistent record of metadata, but it also serves as a logical time line that deﬁnes the order of concurrent operations. Files and chunks, as well as their versions (see Section 4.5), are all uniquely and eternally identiﬁed by the logical times at which they were created.
Since the operation log is critical, we must store it reliably and not make changes visible to clients until metadata changes are made persistent. Otherwise, we eﬀectively lose the whole ﬁle system or recent client operations even if the chunks themselves survive. Therefore, we replicate it on multiple remote machines and respond to a client operation only after ﬂushing the corresponding log record to disk both locally and remotely. The master batches several log records together before ﬂushing thereby reducing the impact of ﬂushing and replication on overall system throughput.
The master recovers its ﬁle system state by replaying the operation log. To minimize startup time, we must keep the log small. The master checkpoints its state whenever the log grows beyond a certain size so that it can recover by loading the latest checkpoint from local disk and replaying only the
limited number of log records after that. The checkpoint is in a compact B-tree like form that can be directly mapped into memory and used for namespace lookup without extra parsing. This further speeds up recovery and improves availability.
Because building a checkpoint can take a while, the master’s internal state is structured in such a way that a new checkpoint can be created without delaying incoming mutations. The master switches to a new log ﬁle and creates the new checkpoint in a separate thread. The new checkpoint includes all mutations before the switch. It can be created in a minute or so for a cluster with a few million ﬁles. When completed, it is written to disk both locally and remotely.
Recovery needs only the latest complete checkpoint and subsequent log ﬁles. Older checkpoints and log ﬁles can be freely deleted, though we keep a few around to guard against catastrophes. A failure during checkpointing does not aﬀect correctness because the recovery code detects and skips incomplete checkpoints.
GFS has a relaxed consistency model that supports our highly distributed applications well but remains relatively simple and eﬃcient to implement. We now discuss GFS’s guarantees and what they mean to applications. We also highlight how GFS maintains these guarantees but leave the details to other parts of the paper.
File namespace mutations (e.g., ﬁle creation) are atomic. these operations (Section 2.6.3).
The state of a ﬁle region after a data mutation depends on the type of mutation, whether it succeeds or fails, and whether there are concurrent mutations. Table 1 summarizes the result. A ﬁle region is consistent if all clients will always see the same data, regardless of which replicas they read from. A region is deﬁned after a ﬁle data mutation if it is consistent and clients will see what the mutation writes in its entirety. When a mutation succeeds without interference from concurrent writers, the aﬀected region is deﬁned (and by implication consistent): all clients will always see what the mutation has written. Concurrent successful mutations leave the region undeﬁned but consistent: all clients see the same data, but it may not reﬂect what any one mutation has written. Typically, it consists of mingled fragments from multiple mutations. A failed mutation makes the region inconsistent (hence also undeﬁned): diﬀerent clients may see diﬀerent data at diﬀerent times. We describe below how our applications can distinguish deﬁned regions from undeﬁned
regions. The applications do not need to further distinguish between diﬀerent kinds of undeﬁned regions.
Data mutations may be writes or record appends. A write causes data to be written at an application-speciﬁed ﬁle oﬀset. A record append causes data (the “record”) to be appended atomically at least once even in the presence of concurrent mutations, but at an oﬀset of GFS’s choosing (Section 3.3). (In contrast, a “regular” append is merely a write at an oﬀset that the client believes to be the current end of ﬁle.) The oﬀset is returned to the client and marks the beginning of a deﬁned region that contains the record. and are typically dwarfed by the amount of user data.
After a sequence of successful mutations, the mutated ﬁle region is guaranteed to be deﬁned and contain the data written by the last mutation. GFS achieves this by (a) applying mutations to a chunk in the same order on all its replicas (Section 3.1), and (b) using chunk version numbers to detect any replica that has become stale because it has missed mutations while its chunkserver was down (Section 4.5). Stale replicas will never be involved in a mutation or given to clients asking the master for chunk locations. They are garbage collected at the earliest opportunity.
Since clients cache chunk locations, they may read from a stale replica before that information is refreshed. This window is limited by the cache entry’s timeout and the next open of the ﬁle, which purges from the cache all chunk information for that ﬁle. Moreover, as most of our ﬁles are append-only, a stale replica usually returns a premature end of chunk rather than outdated data. When a reader retries and contacts the master, it will immediately get current chunk locations.
Long after a successful mutation, component failures can of course still corrupt or destroy data. GFS identiﬁes failed chunkservers by regular handshakes between master and all chunkservers and detects data corruption by checksumming (Section 5.2). Once a problem surfaces, the data is restored from valid replicas as soon as possible (Section 4.3). A chunk is lost irreversibly only if all its replicas are lost before GFS can react, typically within minutes. Even in this case, it becomes unavailable, not corrupted: applications receive clear errors rather than corrupt data.
GFS applications can accommodate the relaxed consistency model with a few simple techniques already needed for other purposes: relying on appends rather than overwrites, checkpointing, and writing self-validating, self-identifying records.
Practically all our applications mutate ﬁles by appending rather than overwriting. In one typical use, a writer generates a ﬁle from beginning to end. It atomically renames the ﬁle to a permanent name after writing all the data, or periodically checkpoints how much has been successfully written. Checkpoints may also include application-level checksums. Readers verify and process only the ﬁle region up to the last checkpoint, which is known to be in the deﬁned state. Regardless of consistency and concurrency issues, this approach has served us well. Appending is far more eﬃcient and more resilient to application failures than random writes. Checkpointing allows writers to restart incrementally and keeps readers from processing successfully written
ﬁle data that is still incomplete from the application’s perspective.
In the other typical use, many writers concurrently append to a ﬁle for merged results or as a producer-consumer queue. Record append’s append-at-least-once semantics preserves each writer’s output. Readers deal with the occasional padding and duplicates as follows. Each record prepared by the writer contains extra information like checksums so that its validity can be veriﬁed. A reader can identify and discard extra padding and record fragments using the checksums. delivered to the record reader.
We designed the system to minimize the master’s involvement in all operations. With that background, we now describe how the client, master, and chunkservers interact to implement data mutations, atomic record append, and snapshot.
A mutation is an operation that changes the contents or metadata of a chunk such as a write or an append operation. Each mutation is performed at all the chunk’s replicas. by the primary.
The lease mechanism is designed to minimize management overhead at the master. A lease has an initial timeout of 60 seconds. However, as long as the chunk is being mutated, the primary can request and typically receive extensions from the master indeﬁnitely. These extension requests and grants are piggybacked on the HeartBeat messages regularly exchanged between the master and all chunkservers. lease to another replica after the old lease expires.
In Figure 2, we illustrate this process by following the
1. The client asks the master which chunkserver holds the current lease for the chunk and the locations of the other replicas. If no one has a lease, the master grants one to a replica it chooses (not shown).
2. The master replies with the identity of the primary and the locations of the other (secondary) replicas. The client caches this data for future mutations. It needs to contact the master again only when the primary
becomes unreachable or replies that it no longer holds a lease.
3. The client pushes the data to all the replicas. A client can do so in any order. Each chunkserver will store the data in an internal LRU buﬀer cache until the data is used or aged out. By decoupling the data ﬂow from the control ﬂow, we can improve performance by scheduling the expensive data ﬂow based on the network topology regardless of which chunkserver is the primary. Section 3.2 discusses this further.
4. Once all the replicas have acknowledged receiving the data, the client sends a write request to the primary. in serial number order.
5. The primary forwards the write request to all secondary replicas. Each secondary replica applies mutations in the same serial number order assigned by the primary.
7. The primary replies to the client. Any errors encountered at any of the replicas are reported to the client. cas. mutation. ginning of the write.
If a write by the application is large or straddles a chunk boundary, GFS client code breaks it down into multiple write operations. They all follow the control ﬂow described above but may be interleaved with and overwritten by concurrent operations from other clients. Therefore, the shared
ﬁle region may end up containing fragments from diﬀerent clients, although the replicas will be identical because the individual operations are completed successfully in the same order on all replicas. This leaves the ﬁle region in consistent but undeﬁned state as noted in Section 2.7.
We decouple the ﬂow of data from the ﬂow of control to use the network eﬃciently. While control ﬂows from the client to the primary and then to all secondaries, data is pushed linearly along a carefully picked chain of chunkservers in a pipelined fashion. Our goals are to fully utilize each machine’s network bandwidth, avoid network bottlenecks and high-latency links, and minimize the latency to push through all the data.
To fully utilize each machine’s network bandwidth, the data is pushed linearly along a chain of chunkservers rather than distributed in some other topology (e.g., tree). Thus, each machine’s full outbound bandwidth is used to transfer the data as fast as possible rather than divided among multiple recipients.
To avoid network bottlenecks and high-latency links (e.g., inter-switch links are often both) as much as possible, each machine forwards the data to the “closest” machine in the network topology that has not received it. Suppose the client is pushing data to chunkservers S1 through S4. IP addresses.
Finally, we minimize latency by pipelining the data transfer over TCP connections. Once a chunkserver receives some data, it starts forwarding immediately. Pipelining is especially helpful to us because we use a switched network with full-duplex links. Sending the data immediately does not reduce the receive rate. Without network congestion, the ideal elapsed time for transferring B bytes to R replicas is B/T + RL where T is the network throughput and L is latency to transfer bytes between two machines. Our network links are typically 100 Mbps (T ), and L is far below 1 ms. Therefore, 1 MB can ideally be distributed in about 80 ms.
GFS provides an atomic append operation called record append. In a traditional write, the client speciﬁes the oﬀset at which data is to be written. Concurrent writes to the same region are not serializable: the region may end up containing data fragments from multiple clients. In a record append, however, the client speciﬁes only the data. GFS appends it to the ﬁle at least once atomically (i.e., as one continuous sequence of bytes) at an oﬀset of GFS’s choosing and returns that oﬀset to the client. This is similar to writing to a ﬁle opened in O APPEND mode in Unix without the race conditions when multiple writers do so concurrently.
Record append is heavily used by our distributed applications in which many clients on diﬀerent machines append to the same ﬁle concurrently. Clients would need additional complicated and expensive synchronization, for example through a distributed lock manager, if they do so with traditional writes.
serve as multiple-producer/single-consumer queues or contain merged results from many diﬀerent clients.
Record append is a kind of mutation and follows the control ﬂow in Section 3.1 with only a little extra logic at the primary. The client pushes the data to all replicas of the last chunk of the ﬁle Then, it sends its request to the primary. The primary checks to see if appending the record to the current chunk would cause the chunk to exceed the maximum size (64 MB). If so, it pads the chunk to the maximum size, tells secondaries to do the same, and replies to the client indicating that the operation should be retried on the next chunk. (Record append is restricted to be at most one-fourth of the maximum chunk size to keep worstcase fragmentation at an acceptable level.) If the record ﬁts within the maximum size, which is the common case, the primary appends the data to its replica, tells the secondaries to write the data at the exact oﬀset where it has, and ﬁnally replies success to the client.
If a record append fails at any replica, the client retries the operation. As a result, replicas of the same chunk may contain diﬀerent data possibly including duplicates of the same record in whole or in part. GFS does not guarantee that all replicas are bytewise identical. It only guarantees that the data is written at least once as an atomic unit. This property follows readily from the simple observation that for the operation to report success, the data must have been written at the same oﬀset on all replicas of some chunk. Furthermore, after this, all replicas are at least as long as the end of record and therefore any future record will be assigned a higher oﬀset or a diﬀerent chunk even if a diﬀerent replica later becomes the primary. In terms of our consistency guarantees, the regions in which successful record append operations have written their data are deﬁned (hence consistent), whereas intervening regions are inconsistent (hence undeﬁned). Our applications can deal with inconsistent regions as we discussed in Section 2.7.2.
The snapshot operation makes a copy of a ﬁle or a directory tree (the “source”) almost instantaneously, while minimizing any interruptions of ongoing mutations. Our users use it to quickly create branch copies of huge data sets (and often copies of those copies, recursively), or to checkpoint the current state before experimenting with changes that can later be committed or rolled back easily.
Like AFS [5], we use standard copy-on-write techniques to implement snapshots. When the master receives a snapshot request, it ﬁrst revokes any outstanding leases on the chunks in the ﬁles it is about to snapshot. This ensures that any subsequent writes to these chunks will require an interaction with the master to ﬁnd the lease holder. This will give the master an opportunity to create a new copy of the chunk ﬁrst.
After the leases have been revoked or have expired, the master logs the operation to disk. It then applies this log record to its in-memory state by duplicating the metadata for the source ﬁle or directory tree. The newly created snapshot ﬁles point to the same chunks as the source ﬁles.
The ﬁrst time a client wants to write to a chunk C after the snapshot operation, it sends a request to the master to ﬁnd the current lease holder. The master notices that the reference count for chunk C is greater than one. It defers replying to the client request and instead picks a new chunk
handle C’. It then asks each chunkserver that has a current replica of C to create a new chunk called C’. By creating the new chunk on the same chunkservers as the original, we ensure that the data can be copied locally, not over the network (our disks are about three times as fast as our 100 Mb Ethernet links). From this point, request handling is no different from that for any chunk: the master grants one of the replicas a lease on the new chunk C’ and replies to the client, which can write the chunk normally, not knowing that it has just been created from an existing chunk.
The master executes all namespace operations. In addition, it manages chunk replicas throughout the system: it makes placement decisions, creates new chunks and hence replicas, and coordinates various system-wide activities to keep chunks fully replicated, to balance load across all the chunkservers, and to reclaim unused storage. We now discuss each of these topics.
Many master operations can take a long time: for example, a snapshot operation has to revoke chunkserver leases on all chunks covered by the snapshot. We do not want to delay other master operations while they are running. Therefore, we allow multiple operations to be active and use locks over regions of the namespace to ensure proper serialization.
Unlike many traditional ﬁle systems, GFS does not have a per-directory data structure that lists all the ﬁles in that directory. Nor does it support aliases for the same ﬁle or directory (i.e, hard or symbolic links in Unix terms). GFS logically represents its namespace as a lookup table mapping full pathnames to metadata. With preﬁx compression, this table can be eﬃciently represented in memory. Each node in the namespace tree (either an absolute ﬁle name or an absolute directory name) has an associated read-write lock. a ﬁle or directory depending on the operation.
We now illustrate how this locking mechanism can prevent a ﬁle /home/user/foo from being created while /home/user is being snapshotted to /save/user. The snapshot operation acquires read locks on /home and /save, and write locks on /home/user and /save/user. The ﬁle creation acquires read locks on /home and /home/user, and a write lock on /home/user/foo. The two operations will be serialized properly because they try to obtain conﬂicting locks on /home/user. File creation does not require a write lock on the parent directory because there is no “directory”, or inode-like, data structure to be protected from modiﬁcation. directory from deletion.
One nice property of this locking scheme is that it allows concurrent mutations in the same directory. For example, multiple ﬁle creations can be executed concurrently in the same directory: each acquires a read lock on the directory name and a write lock on the ﬁle name. The read lock on the directory name suﬃces to prevent the directory from being deleted, renamed, or snapshotted. The write locks on
ﬁle names serialize attempts to create a ﬁle with the same name twice.
Since the namespace can have many nodes, read-write lock objects are allocated lazily and deleted once they are not in use. Also, locks are acquired in a consistent total order to prevent deadlock: they are ﬁrst ordered by level in the namespace tree and lexicographically within the same level.
A GFS cluster is highly distributed at more levels than one. It typically has hundreds of chunkservers spread across many machine racks. These chunkservers in turn may be accessed from hundreds of clients from the same or diﬀerent racks. Communication between two machines on diﬀerent racks may cross one or more network switches. Additionally, bandwidth into or out of a rack may be less than the aggregate bandwidth of all the machines within the rack. tribute data for scalability, reliability, and availability.
The chunk replica placement policy serves two purposes: maximize data reliability and availability, and maximize network bandwidth utilization. For both, it is not enough to spread replicas across machines, which only guards against disk or machine failures and fully utilizes each machine’s network bandwidth. We must also spread chunk replicas across racks. This ensures that some replicas of a chunk will survive and remain available even if an entire rack is damaged or oﬄine (for example, due to failure of a shared resource like a network switch or power circuit). It also means that traﬃc, especially reads, for a chunk can exploit the aggregate bandwidth of multiple racks. On the other hand, write traﬃc has to ﬂow through multiple racks, a tradeoﬀ we make willingly.
When the master creates a chunk, it chooses where to place the initially empty replicas. It considers several factors. (1) We want to place new replicas on chunkservers with below-average disk space utilization. Over time this will equalize disk utilization across chunkservers. (2) We want to limit the number of “recent” creations on each chunkserver. want to spread replicas of a chunk across racks.
The master re-replicates a chunk as soon as the number of available replicas falls below a user-speciﬁed goal. This could happen for various reasons: a chunkserver becomes unavailable, it reports that its replica may be corrupted, one of its disks is disabled because of errors, or the replication goal is increased. Each chunk that needs to be re-replicated is prioritized based on several factors. One is how far it is from its replication goal. For example, we give higher priority to a chunk that has lost two replicas than to a chunk that has lost only one. In addition, we prefer to ﬁrst re-replicate chunks for live ﬁles as opposed to chunks that belong to recently deleted ﬁles (see Section 4.4). Finally, to minimize the impact of failures on running applications, we boost the priority of any chunk that is blocking client progress.
The master picks the highest priority chunk and “clones” it by instructing some chunkserver to copy the chunk data directly from an existing valid replica. The new replica is placed with goals similar to those for creation: equalizing disk space utilization, limiting active clone operations on any single chunkserver, and spreading replicas across racks. source chunkserver.
Finally, the master rebalances replicas periodically: it examines the current replica distribution and moves replicas for better disk space and load balancing. Also through this process, the master gradually ﬁlls up a new chunkserver rather than instantly swamps it with new chunks and the heavy write traﬃc that comes with them. The placement criteria for the new replica are similar to those discussed above. In addition, the master must also choose which existing replica to remove. to equalize disk space usage.
After a ﬁle is deleted, GFS does not immediately reclaim the available physical storage. It does so only lazily during regular garbage collection at both the ﬁle and chunk levels. and more reliable.
When a ﬁle is deleted by the application, the master logs the deletion immediately just like other changes. However instead of reclaiming resources immediately, the ﬁle is just renamed to a hidden name that includes the deletion timestamp. During the master’s regular scan of the ﬁle system namespace, it removes any such hidden ﬁles if they have existed for more than three days (the interval is conﬁgurable). name and can be undeleted by renaming it back to normal. to all its chunks.
In a similar regular scan of the chunk namespace, the master identiﬁes orphaned chunks (i.e., those not reachable from any ﬁle) and erases the metadata for those chunks. In a HeartBeat message regularly exchanged with the master, each chunkserver reports a subset of the chunks it has, and the master replies with the identity of all chunks that are no longer present in the master’s metadata. The chunkserver is free to delete its replicas of such chunks.
Although distributed garbage collection is a hard problem that demands complicated solutions in the context of programming languages, it is quite simple in our case. We can easily identify all references to chunks: they are in the ﬁleto-chunk mappings maintained exclusively by the master. Linux ﬁles under designated directories on each chunkserver.
The garbage collection approach to storage reclamation oﬀers several advantages over eager deletion. First, it is simple and reliable in a large-scale distributed system where component failures are common. Chunk creation may succeed on some chunkservers but not others, leaving replicas that the master does not know exist. Replica deletion messages may be lost, and the master has to remember to resend them across failures, both its own and the chunkserver’s. safety net against accidental, irreversible deletion.
In our experience, the main disadvantage is that the delay sometimes hinders user eﬀort to ﬁne tune usage when storage is tight. Applications that repeatedly create and delete temporary ﬁles may not be able to reuse the storage right away. We address these issues by expediting storage reclamation if a deleted ﬁle is explicitly deleted again. We also allow users to apply diﬀerent replication and reclamation policies to diﬀerent parts of the namespace. For example, users can specify that all the chunks in the ﬁles within some directory tree are to be stored without replication, and any deleted ﬁles are immediately and irrevocably removed from the ﬁle system state.
Chunk replicas may become stale if a chunkserver fails and misses mutations to the chunk while it is down. For each chunk, the master maintains a chunk version number to distinguish between up-to-date and stale replicas.
Whenever the master grants a new lease on a chunk, it increases the chunk version number and informs the up-todate replicas. The master and these replicas all record the new version number in their persistent state. This occurs before any client is notiﬁed and therefore before it can start writing to the chunk. If another replica is currently unavailable, its chunk version number will not be advanced. The master will detect that this chunkserver has a stale replica when the chunkserver restarts and reports its set of chunks and their associated version numbers. If the master sees a version number greater than the one in its records, the master assumes that it failed when granting the lease and so takes the higher version to be up-to-date.
The master removes stale replicas in its regular garbage collection. Before that, it eﬀectively considers a stale replica not to exist at all when it replies to client requests for chunk information. As another safeguard, the master includes the chunk version number when it informs clients which chunkserver holds a lease on a chunk or when it instructs a chunkserver to read the chunk from another chunkserver in a cloning operation. The client or the chunkserver veriﬁes the version number when it performs the operation so that it is always accessing up-to-date data.
5. FAULT TOLERANCE AND DIAGNOSIS One of our greatest challenges in designing the system is dealing with frequent component failures. The quality and
quantity of components together make these problems more the norm than the exception: we cannot completely trust the machines, nor can we completely trust the disks. Component failures can result in an unavailable system or, worse, corrupted data. We discuss how we meet these challenges and the tools we have built into the system to diagnose problems when they inevitably occur.
Among hundreds of servers in a GFS cluster, some are bound to be unavailable at any given time. We keep the overall system highly available with two simple yet eﬀective strategies: fast recovery and replication.
Both the master and the chunkserver are designed to restore their state and start in seconds no matter how they terminated. In fact, we do not distinguish between normal and abnormal termination; servers are routinely shut down just by killing the process. Clients and other servers experience a minor hiccup as they time out on their outstanding requests, reconnect to the restarted server, and retry. Section 6.2.2 reports observed startup times.
As discussed earlier, each chunk is replicated on multiple chunkservers on diﬀerent racks. Users can specify diﬀerent replication levels for diﬀerent parts of the ﬁle namespace. than small random writes.
The master state is replicated for reliability. Its operation log and checkpoints are replicated on multiple machines. A mutation to the state is considered committed only after its log record has been ﬂushed to disk locally and on all master replicas. For simplicity, one master process remains in charge of all mutations as well as background activities such as garbage collection that change the system internally. log. Clients use only the canonical name of the master (e.g. master is relocated to another machine.
Moreover, “shadow” masters provide read-only access to the ﬁle system even when the primary master is down. They are shadows, not mirrors, in that they may lag the primary slightly, typically fractions of a second. They enhance read availability for ﬁles that are not being actively mutated or applications that do not mind getting slightly stale results.
stale within short windows is ﬁle metadata, like directory contents or access control information.
To keep itself informed, a shadow master reads a replica of the growing operation log and applies the same sequence of changes to its data structures exactly as the primary does. create and delete replicas.
Each chunkserver uses checksumming to detect corruption of stored data. Given that a GFS cluster often has thousands of disks on hundreds of machines, it regularly experiences disk failures that cause data corruption or loss on both the read and write paths. (See Section 7 for one cause.) We can recover from corruption using other chunk replicas, but it would be impractical to detect corruption by comparing replicas across chunkservers. Moreover, divergent replicas may be legal: the semantics of GFS mutations, in particular atomic record append as discussed earlier, does not guarantee identical replicas. Therefore, each chunkserver must independently verify the integrity of its own copy by maintaining checksums.
A chunk is broken up into 64 KB blocks. Each has a corresponding 32 bit checksum. Like other metadata, checksums are kept in memory and stored persistently with logging, separate from user data.
For reads, the chunkserver veriﬁes the checksum of data blocks that overlap the read range before returning any data to the requester, whether a client or another chunkserver. other machines. reported the mismatch to delete its replica.
Checksumming has little eﬀect on read performance for several reasons. Since most of our reads span at least a few blocks, we need to read and checksum only a relatively small amount of extra data for veriﬁcation. GFS client code further reduces this overhead by trying to align reads at checksum block boundaries. Moreover, checksum lookups and comparison on the chunkserver are done without any I/O, and checksum calculation can often be overlapped with I/Os.
Checksum computation is heavily optimized for writes that append to the end of a chunk (as opposed to writes that overwrite existing data) because they are dominant in our workloads. We just incrementally update the checksum for the last partial checksum block, and compute new checksums for any brand new checksum blocks ﬁlled by the append. Even if the last partial checksum block is already corrupted and we fail to detect it now, the new checksum value will not match the stored data, and the corruption will be detected as usual when the block is next read.
In contrast, if a write overwrites an existing range of the chunk, we must read and verify the ﬁrst and last blocks of the range being overwritten, then perform the write, and
ﬁnally compute and record the new checksums. in the regions not being overwritten.
During idle periods, chunkservers can scan and verify the contents of inactive chunks. This allows us to detect corruption in chunks that are rarely read. Once the corruption is detected, the master can create a new uncorrupted replica and delete the corrupted replica. This prevents an inactive but corrupted chunk replica from fooling the master into thinking that it has enough valid replicas of a chunk.
Extensive and detailed diagnostic logging has helped immeasurably in problem isolation, debugging, and performance analysis, while incurring only a minimal cost. Without logs, it is hard to understand transient, non-repeatable interactions between machines. GFS servers generate diagnostic logs that record many signiﬁcant events (such as chunkservers going up and down) and all RPC requests and replies. These diagnostic logs can be freely deleted without aﬀecting the correctness of the system. However, we try to keep these logs around as far as space permits.
The RPC logs include the exact requests and responses sent on the wire, except for the ﬁle data being read or written. By matching requests with replies and collating RPC records on diﬀerent machines, we can reconstruct the entire interaction history to diagnose a problem. The logs also serve as traces for load testing and performance analysis.
The performance impact of logging is minimal (and far outweighed by the beneﬁts) because these logs are written sequentially and asynchronously. The most recent events are also kept in memory and available for continuous online monitoring.
In this section we present a few micro-benchmarks to illustrate the bottlenecks inherent in the GFS architecture and implementation, and also some numbers from real clusters in use at Google.
We measured performance on a GFS cluster consisting of one master, two master replicas, 16 chunkservers, and 16 clients. Note that this conﬁguration was set up for ease of testing. Typical clusters have hundreds of chunkservers and hundreds of clients.
All the machines are conﬁgured with dual 1.4 GHz PIII processors, 2 GB of memory, two 80 GB 5400 rpm disks, and a 100 Mbps full-duplex Ethernet connection to an HP 2524 switch. All 19 GFS server machines are connected to one switch, and all 16 client machines to the other. The two switches are connected with a 1 Gbps link.
N clients read simultaneously from the ﬁle system. Each client reads a randomly selected 4 MB region from a 320 GB ﬁle set. This is repeated 256 times so that each client ends up reading 1 GB of data. The chunkservers taken together have only 32 GB of memory, so we expect at most a 10% hit rate in the Linux buﬀer cache. Our results should be close to cold cache results.
Figure 3(a) shows the aggregate read rate for N clients and its theoretical limit. The limit peaks at an aggregate of 125 MB/s when the 1 Gbps link between the two switches is saturated, or 12.5 MB/s per client when its 100 Mbps network interface gets saturated, whichever applies. The observed read rate is 10 MB/s, or 80% of the per-client limit, when just one client is reading. The aggregate read rate reaches 94 MB/s, about 75% of the 125 MB/s link limit, for 16 readers, or 6 MB/s per client. The eﬃciency drops from 80% to 75% because as the number of readers increases, so does the probability that multiple readers simultaneously read from the same chunkserver.
N clients write simultaneously to N distinct ﬁles. Each client writes 1 GB of data to a new ﬁle in a series of 1 MB writes. The aggregate write rate and its theoretical limit are shown in Figure 3(b). The limit plateaus at 67 MB/s because we need to write each byte to 3 of the 16 chunkservers, each with a 12.5 MB/s input connection.
The write rate for one client is 6.3 MB/s, about half of the limit. The main culprit for this is our network stack. It does not interact very well with the pipelining scheme we use for pushing data to chunk replicas. Delays in propagating data from one replica to another reduce the overall write rate.
Aggregate write rate reaches 35 MB/s for 16 clients (or 2.2 MB/s per client), about half the theoretical limit. As in the case of reads, it becomes more likely that multiple clients write concurrently to the same chunkserver as the number of clients increases. Moreover, collision is more likely for 16 writers than for 16 readers because each write involves three diﬀerent replicas.
Writes are slower than we would like. In practice this has not been a major problem because even though it increases the latencies as seen by individual clients, it does not signiﬁcantly aﬀect the aggregate write bandwidth delivered by the system to a large number of clients.
Figure 3(c) shows record append performance. N clients append simultaneously to a single ﬁle. Performance is limited by the network bandwidth of the chunkservers that store the last chunk of the ﬁle, independent of the number of clients. It starts at 6.0 MB/s for one client and drops to 4.8 MB/s for 16 clients, mostly due to congestion and variances in network transfer rates seen by diﬀerent clients. the chunkservers for another ﬁle are busy.
We now examine two clusters in use within Google that are representative of several others like them. Cluster A is used regularly for research and development by over a hundred engineers. A typical task is initiated by a human user and runs up to several hours. It reads through a few MBs to a few TBs of data, transforms or analyzes the data, and writes the results back to the cluster. Cluster B is primarily used for production data processing. The tasks last much
longer and continuously generate and process multi-TB data sets with only occasional human intervention. In both cases, a single “task” consists of many processes on many machines reading and writing many ﬁles simultaneously.
As shown by the ﬁrst ﬁve entries in the table, both clusters have hundreds of chunkservers, support many TBs of disk space, and are fairly but not completely full. “Used space” includes all chunk replicas. Virtually all ﬁles are replicated three times. Therefore, the clusters store 18 TB and 52 TB of ﬁle data respectively.
The two clusters have similar numbers of ﬁles, though B has a larger proportion of dead ﬁles, namely ﬁles which were deleted or replaced by a new version but whose storage have not yet been reclaimed. It also has more chunks because its ﬁles tend to be larger.
The chunkservers in aggregate store tens of GBs of metadata, mostly the checksums for 64 KB blocks of user data. chunk version number discussed in Section 4.5.
The metadata kept at the master is much smaller, only tens of MBs, or about 100 bytes per ﬁle on average. This agrees with our assumption that the size of the master’s memory does not limit the system’s capacity in practice. implementing copy-on-write.
Each individual server, both chunkservers and the master, has only 50 to 100 MB of metadata. Therefore recovery is fast: it takes only a few seconds to read this metadata from disk before the server is able to answer queries. However, the master is somewhat hobbled for a period – typically 30 to 60 seconds – until it has fetched chunk location information from all chunkservers.
Table 3 shows read and write rates for various time periods. Both clusters had been up for about one week when these measurements were taken.
The average write rate was less than 30 MB/s since the restart. When we took these measurements, B was in the middle of a burst of write activity generating about 100 MB/s of data, which produced a 300 MB/s network load because writes are propagated to three replicas.
Figure 3: Aggregate Throughputs. Top curves show theoretical limits imposed by our network topology. Bottom curves show measured throughputs. They have error bars that show 95% conﬁdence intervals, which are illegible in some cases because of low variance in measurements.
Read rate (last minute) Read rate (last hour) Read rate (since restart) Write rate (last minute) Write rate (last hour) Write rate (since restart) Master ops (last minute) Master ops (last hour) Master ops (since restart)
The read rates were much higher than the write rates. 1300 MB/s, but its applications were using just 380 MB/s.
Table 3 also shows that the rate of operations sent to the master was around 200 to 500 operations per second. The master can easily keep up with this rate, and therefore is not a bottleneck for these workloads.
In an earlier version of GFS, the master was occasionally a bottleneck for some workloads. It spent most of its time sequentially scanning through large directories (which contained hundreds of thousands of ﬁles) looking for particular ﬁles. We have since changed the master data structures to allow eﬃcient binary searches through the namespace. tures.
After a chunkserver fails, some chunks will become underreplicated and must be cloned to restore their replication levels. The time it takes to restore all such chunks depends on the amount of resources. In one experiment, we killed a single chunkserver in cluster B. The chunkserver had about
15,000 chunks containing 600 GB of data. To limit the impact on running applications and provide leeway for scheduling decisions, our default parameters limit this cluster to 91 concurrent clonings (40% of the number of chunkservers) where each clone operation is allowed to consume at most 6.25 MB/s (50 Mbps). All chunks were restored in 23.2 minutes, at an eﬀective replication rate of 440 MB/s.
In another experiment, we killed two chunkservers each with roughly 16,000 chunks and 660 GB of data. This double failure reduced 266 chunks to having a single replica. These 266 chunks were cloned at a higher priority, and were all restored to at least 2x replication within 2 minutes, thus putting the cluster in a state where it could tolerate another chunkserver failure without data loss.
In this section, we present a detailed breakdown of the workloads on two GFS clusters comparable but not identical to those in Section 6.2. Cluster X is for research and development while cluster Y is for production data processing.
These results include only client originated requests so that they reﬂect the workload generated by our applications for the ﬁle system as a whole. They do not include interserver requests to carry out client requests or internal background activities, such as forwarded writes or rebalancing. chines.
One should be careful not to overly generalize from our workload. Since Google completely controls both GFS and its applications, the applications tend to be tuned for GFS, and conversely GFS is designed for these applications. Such mutual inﬂuence may also exist between general applications
2.3 1.9 < .1 31.6 0.4 < .1 4.2 7.7 < .1 2.2
< .1 < .1 < .1 < .1 < .1 13.8 3.9 < .1 < .1 < .1 2.4 5.9 16.5 0.2 < .1 0.8 0.6 3.4 7.7 < .1 1.4 0.3
Table 4: Operations Breakdown by Size (%). For reads, the size is the amount of data actually read and transferred, rather than the amount requested.
and ﬁle systems, but the eﬀect is likely more pronounced in our case.
Table 4 shows the distribution of operations by size. Read sizes exhibit a bimodal distribution. The small reads (under 64 KB) come from seek-intensive clients that look up small pieces of data within huge ﬁles. The large reads (over 512 KB) come from long sequential reads through entire ﬁles.
A signiﬁcant number of reads return no data at all in cluster Y. Our applications, especially those in the production systems, often use ﬁles as producer-consumer queues. Producers append concurrently to a ﬁle while a consumer reads the end of ﬁle. Occasionally, no data is returned when the consumer outpaces the producers. Cluster X shows this less often because it is usually used for short-lived data analysis tasks rather than long-lived distributed applications.
Write sizes also exhibit a bimodal distribution. The large writes (over 256 KB) typically result from signiﬁcant buﬀering within the writers. Writers that buﬀer less data, checkpoint or synchronize more often, or simply generate less data account for the smaller writes (under 64 KB).
As for record appends, cluster Y sees a much higher percentage of large record appends than cluster X does because our production systems, which use cluster Y, are more aggressively tuned for GFS.
Table 5 shows the total amount of data transferred in operations of various sizes. For all kinds of operations, the larger operations (over 256 KB) generally account for most of the bytes transferred. Small reads (under 64 KB) do transfer a small but signiﬁcant portion of the read data because of the random seek workload.
Record appends are heavily used especially in our production systems. For cluster X, the ratio of writes to record appends is 108:1 by bytes transferred and 8:1 by operation counts. For cluster Y, used by the production systems, the ratios are 3.7:1 and 2.5:1 respectively. Moreover, these ratios suggest that for both clusters record appends tend to be larger than writes. For cluster X, however, the overall usage of record append during the measured period is fairly low and so the results are likely skewed by one or two applications with particular buﬀer size choices.
As expected, our data mutation workload is dominated by appending rather than overwriting. We measured the amount of data overwritten on primary replicas. This ap-
Table 5: Bytes Transferred Breakdown by Operation Size (%). For reads, the size is the amount of data actually read and transferred, rather than the amount requested. The two may diﬀer if the read attempts to read beyond end of ﬁle, which by design is not uncommon in our workloads.
proximates the case where a client deliberately overwrites previous written data rather than appends new data. For cluster X, overwriting accounts for under 0.0001% of bytes mutated and under 0.0003% of mutation operations. For cluster Y, the ratios are both 0.05%. Although this is minute, it is still higher than we expected. It turns out that most of these overwrites came from client retries due to errors or timeouts. They are not part of the workload per se but a consequence of the retry mechanism.
Table 6 shows the breakdown by type of requests to the master. Most requests ask for chunk locations (FindLocation) for reads and lease holder information (FindLeaseLocker) for data mutations.
Clusters X and Y see signiﬁcantly diﬀerent numbers of Delete requests because cluster Y stores production data sets that are regularly regenerated and replaced with newer versions. Some of this diﬀerence is further hidden in the diﬀerence in Open requests because an old version of a ﬁle may be implicitly deleted by being opened for write from scratch (mode “w” in Unix open terminology).
FindMatchingFiles is a pattern matching request that supports “ls” and similar ﬁle system operations. Unlike other requests for the master, it may process a large part of the namespace and so may be expensive. Cluster Y sees it much more often because automated data processing tasks tend to examine parts of the ﬁle system to understand global application state. In contrast, cluster X’s applications are under more explicit user control and usually know the names of all needed ﬁles in advance.
In the process of building and deploying GFS, we have experienced a variety of issues, some operational and some technical.
Initially, GFS was conceived as the backend ﬁle system for our production systems. Over time, the usage evolved to include research and development tasks. It started with little support for things like permissions and quotas but now includes rudimentary forms of these. While production systems are well disciplined and controlled, users sometimes are not. More infrastructure is required to keep users from interfering with one another.
Some of our biggest problems were disk and Linux related. these protocol mismatches.
Earlier we had some problems with Linux 2.2 kernels due to the cost of fsync(). Its cost is proportional to the size of the ﬁle rather than the size of the modiﬁed portion. This was a problem for our large operation logs especially before we implemented checkpointing. We worked around this for a time by using synchronous writes and eventually migrated to Linux 2.4.
Another Linux problem was a single reader-writer lock which any thread in an address space must hold when it pages in from disk (reader lock) or modiﬁes the address space in an mmap() call (writer lock). We saw transient timeouts in our system under light load and looked hard for resource bottlenecks or sporadic hardware failures. Eventually, we found that this single lock blocked the primary network thread from mapping new data into memory while the disk threads were paging in previously mapped data. replacing mmap() with pread() at the cost of an extra copy. and share the changes with the open source community.
Like other large distributed ﬁle systems such as AFS [5], GFS provides a location independent namespace which enables data to be moved transparently for load balance or fault tolerance. Unlike AFS, GFS spreads a ﬁle’s data across storage servers in a way more akin to xFS [1] and Swift [3] in order to deliver aggregate performance and increased fault tolerance.
As disks are relatively cheap and replication is simpler than more sophisticated RAID [9] approaches, GFS currently uses only replication for redundancy and so consumes more raw storage than xFS or Swift.
In contrast to systems like AFS, xFS, Frangipani [12], and Intermezzo [6], GFS does not provide any caching below the ﬁle system interface. Our target workloads have little reuse within a single application run because they either stream through a large data set or randomly seek within it and read small amounts of data each time.
Some distributed ﬁle systems like Frangipani, xFS, Minnesota’s GFS[11] and GPFS [10] remove the centralized server
and rely on distributed algorithms for consistency and management. We opt for the centralized approach in order to simplify the design, increase its reliability, and gain ﬂexibility. In particular, a centralized master makes it much easier to implement sophisticated chunk placement and replication policies since the master already has most of the relevant information and controls how it changes. We address fault tolerance by keeping the master state small and fully replicated on other machines. Scalability and high availability (for reads) are currently provided by our shadow master mechanism. Updates to the master state are made persistent by appending to a write-ahead log. Therefore we could adapt a primary-copy scheme like the one in Harp [7] to provide high availability with stronger consistency guarantees than our current scheme.
We are addressing a problem similar to Lustre [8] in terms of delivering aggregate performance to a large number of clients. However, we have simpliﬁed the problem signiﬁcantly by focusing on the needs of our applications rather than building a POSIX-compliant ﬁle system. Additionally, GFS assumes large number of unreliable components and so fault tolerance is central to our design.
GFS most closely resembles the NASD architecture [4]. are required in a production environment.
Unlike Minnesota’s GFS and NASD, we do not seek to alter the model of the storage device. We focus on addressing day-to-day data processing needs for complicated distributed systems with existing commodity components.
The producer-consumer queues enabled by atomic record appends address a similar problem as the distributed queues in River [2]. While River uses memory-based queues distributed across machines and careful data ﬂow control, GFS uses a persistent ﬁle that can be appended to concurrently by many producers. The River model supports m-to-n distributed queues but lacks the fault tolerance that comes with persistent storage, while GFS only supports m-to-1 queues eﬃciently. Multiple consumers can read the same ﬁle, but they must coordinate to partition the incoming load.
The Google File System demonstrates the qualities essential for supporting large-scale data processing workloads on commodity hardware. While some design decisions are speciﬁc to our unique setting, many may apply to data processing tasks of a similar magnitude and cost consciousness. have led to radically diﬀerent points in the design space. interface to improve the overall system.
Our system provides fault tolerance by constant monitoring, replicating crucial data, and fast and automatic recovery. Chunk replication allows us to tolerate chunkserver
architecture. In Proceedings of the 8th Architectural Support for Programming Languages and Operating Systems, pages 92–103, San Jose, California, October 1998.
Nichols, Mahadev Satyanarayanan, Robert Sidebotham, and Michael West. Scale and performance in a distributed ﬁle system. ACM Transactions on Computer Systems, 6(1):51–81, February 1988.
[6] InterMezzo. http://www.inter-mezzo.org, 2003. Paul Johnson, Liuba Shrira, and Michael Williams. 226–238, Paciﬁc Grove, CA, October 1991.
[8] Lustre. http://www.lustreorg, 2003. [9] David A. Patterson, Garth A. Gibson, and Randy H.
Katz. A case for redundant arrays of inexpensive disks (RAID). In Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data, pages 109–116, Chicago, Illinois, September 1988.
shared-disk ﬁle system for large computing clusters. In Proceedings of the First USENIX Conference on File and Storage Technologies, pages 231–244, Monterey, California, January 2002.
[11] Steven R. Soltis, Thomas M. Ruwart, and Matthew T. Park, Maryland, September 1996.
Edward K. Lee. Frangipani: A scalable distributed ﬁle system. In Proceedings of the 16th ACM Symposium on Operating System Principles, pages 224–237, Saint-Malo, France, October 1997.
failures. The frequency of these failures motivated a novel online repair mechanism that regularly and transparently repairs the damage and compensates for lost replicas as soon as possible. Additionally, we use checksumming to detect data corruption at the disk or IDE subsystem level, which becomes all too common given the number of disks in the system.
Our design delivers high aggregate throughput to many concurrent readers and writers performing a variety of tasks. ple, centralized master that does not become a bottleneck. an individual client.
GFS has successfully met our storage needs and is widely used within Google as the storage platform for research and development as well as production data processing. It is an important tool that enables us to continue to innovate and attack problems on the scale of the entire web.
We wish to thank the following people for their contributions to the system or the paper. Brain Bershad (our shepherd) and the anonymous reviewers gave us valuable comments and suggestions. Anurag Acharya, Jeﬀ Dean, and David desJardins contributed to the early design. Fay Chang worked on comparison of replicas across chunkservers. Guy Edjlali worked on storage quota. Markus Gutschke worked on a testing framework and security enhancements. David Kramer worked on performance enhancements. Fay Chang, Urs Hoelzle, Max Ibel, Sharon Perl, Rob Pike, and Debby Wallach commented on earlier drafts of the paper. Many of our colleagues at Google bravely trusted their data to a new ﬁle system and gave us useful feedback. Yoshka helped with early testing.
David Patterson, Drew Roselli, and Randolph Wang. Colorado, December 1995.
Treuhaft, David E. Culler, Joseph M. Hellerstein, David Patterson, and Kathy Yelick. Cluster I/O with River: Making the fast case common. In Proceedings of the Sixth Workshop on Input/Output in Parallel and Distributed Systems (IOPADS ’99), pages 10–22, Atlanta, Georgia, May 1999.
[3] Luis-Felipe Cabrera and Darrell D. E. Long. Swift: Using distributed disk striping to provide high I/O data rates. Computer Systems, 4(4):405–436, 1991.
[4] Garth A. Gibson, David F. Nagle, Khalil Amiri, Jeﬀ
Butler, Fay W. Chang, Howard Gobioﬀ, Charles Hardin, Erik Riedel, David Rochberg, and Jim Zelenka. A cost-eﬀective, high-bandwidth storage
